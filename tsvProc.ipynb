{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "path2Data=\"cities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the POS tags of the tweets in path2Data\n",
    "# Since numbers and such give many unique ''words'' we can specify which to replace\n",
    "# currently replacing '#'-hashtags, '@'-usernames, 'U'-url links, 'E'-emoticons, '$'- numeral , ','-punctuations, 'G'-unknown tag\n",
    "# we are removing some formating symbols eg. \":\" which is tagged to '~'\n",
    "replaceables = ['#', '@', 'U', 'E', '$', ',', 'G']\n",
    "#replaceables = []\n",
    "removables = ['~']\n",
    "def cleanTweet(tweet, tweet_pos):\n",
    "    tweet_l = tweet.split()\n",
    "    tweet_pos_l = tweet_pos.split()\n",
    "\n",
    "    if len(tweet_l) != len(tweet_pos_l):\n",
    "        for i, item in enumerate(tweet_l):\n",
    "            print (tweet_l[i], ',' , tweet_pos_l[i])\n",
    "        \n",
    "    clean_tweet = []\n",
    "    for i, item in enumerate(tweet_l):\n",
    "        #print (item)\n",
    "        #print (tweet_pos_l[i])\n",
    "        if tweet_pos_l[i] in replaceables:\n",
    "            clean_tweet.append(tweet_pos_l[i])\n",
    "        elif tweet_pos_l[i] in removables:\n",
    "            None\n",
    "        else:\n",
    "            clean_tweet.append(item.lower())\n",
    "    \n",
    "    #print (clean_tweet)\n",
    "    return clean_tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenum = 0\n",
    "Vcount=0\n",
    "Vinv={}          # index to word map\n",
    "V={}             # word to index\n",
    "idf={}           # forwardIndex\n",
    "tf={}            # (word, numberOfWords)\n",
    "locRates={}      # HIV rates based on locations\n",
    "#inverseIndex={}  \n",
    "N = 0\n",
    "for file in glob.glob(path2Data+'*.tsv'):\n",
    "    filenum = filenum + 1  #serves as an index for the file name\n",
    "    prefix = file.split('.')[0]\n",
    "    locRates[filenum] = int(prefix.split('_')[-1])\n",
    "    \n",
    "    lines = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    #DEBUG\n",
    "    #print (file + \" file num: \" + str(filenum) + \" num tweets: \" + str(len(lines)) )\n",
    "    \n",
    "    unique_words = set([])\n",
    "    for line in lines:\n",
    "        ll = line.split('\\t')\n",
    "        tweet = ll[0].strip()\n",
    "        tweet_pos = ll[1].strip()\n",
    "        \n",
    "        for word in cleanTweet(tweet, tweet_pos):\n",
    "            if word not in V:\n",
    "                V[word]= Vcount\n",
    "                Vinv[Vcount]=word\n",
    "                Vcount = Vcount + 1\n",
    "            \n",
    "            if V[word] not in idf:\n",
    "                idf[ V[word] ] = []\n",
    "            \n",
    "            if filenum not in tf:\n",
    "                tf[filenum] = {}\n",
    "            \n",
    "            freq = tf[filenum].get(V[word], 0)\n",
    "            tf[filenum][ V[word] ] = freq + 1\n",
    "            \n",
    "            if word not in unique_words:\n",
    "                idf[ V[word] ].append(filenum)\n",
    "                unique_words.add(word)\n",
    "            \n",
    "N = filenum\n",
    "VocabSize=len(V.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabSize:  184183\n",
      "docSize:  1504\n",
      "labels:  1504\n"
     ]
    }
   ],
   "source": [
    "print (\"vocabSize: \", VocabSize)\n",
    "print (\"docSize: \", N)\n",
    "print (\"labels: \", len(locRates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean:  133.54255319148936\n",
      "median:  82.5\n",
      "max:  2084\n",
      "min:  11\n",
      "standard diviation:  160.6118525618946\n"
     ]
    }
   ],
   "source": [
    "from statistics import *\n",
    "mu = mean(locRates.values())\n",
    "print (\"mean: \",  mu )\n",
    "med = median(locRates.values())\n",
    "print (\"median: \",  median(locRates.values()) )\n",
    "print (\"max: \", max(locRates.values()) )\n",
    "print (\"min: \", min(locRates.values()) )\n",
    "#print ( median_low(locRates.values()) )\n",
    "#print ( median_high(locRates.values()) )\n",
    "sigma = stdev(locRates.values())\n",
    "print (\"standard diviation: \", sigma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Version 2: generating Valid Keys for corresponding to the top/bottom HIV rates\n",
    "#sample size is 2 (p*N), p*N for lower and p*N for upper\n",
    "def sampleItems(locRates, p):\n",
    "    items = []\n",
    "    for key in locRates:\n",
    "        items.append((key, locRates[key]))\n",
    "\n",
    "    sorted_locRates = sorted(items, key=lambda student: student[1]) \n",
    "    total_items = len(items)\n",
    "    sampleSize = (int) (p*total_items)\n",
    "    print (\"sample size: \", sampleSize)\n",
    "    \n",
    "    ret = []\n",
    "    lab = {}\n",
    "    for i, item in enumerate(sorted_locRates):\n",
    "        if i < sampleSize:\n",
    "            ret.append(item[0])\n",
    "            lab[item[1]] = 0\n",
    "            \n",
    "        if i >= total_items - sampleSize:\n",
    "            ret.append(item[0])\n",
    "            #lab.append(1)\n",
    "            lab[item[1]] = 1\n",
    "    \n",
    "    print (\"samped items size: \", len(ret))\n",
    "    return (ret,lab)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150.4\n",
      "sample size:  150\n",
      "samped items size:  300\n",
      "300 148\n"
     ]
    }
   ],
   "source": [
    "print (len(locRates)*.1)\n",
    "(validKeys, labels) = sampleItems(locRates, .1)\n",
    "print (len(validKeys), len(labels.keys()))\n",
    "#print (validKeys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Version 1: generating Valid Keys for corresponding to the top/bottom HIV rates Q1 and Q3 standardize\n",
    "# using IQR\n",
    "keys = []\n",
    "newrates = []\n",
    "for key in locRates:\n",
    "    keys.append(key)\n",
    "    newrates.append( (locRates[key] - mu)/sigma )\n",
    "    \n",
    "q1 = stdev(newrates) * -0.67 + mean(newrates)\n",
    "q3 = stdev(newrates) *  0.67 + mean(newrates)\n",
    "\n",
    "validKeys = []\n",
    "labels = []\n",
    "eps = 0.1\n",
    "highCount = 0\n",
    "lowCount = 0\n",
    "for i, rate in enumerate(newrates):\n",
    "    if rate < q1+eps:\n",
    "        #lowRates.append(rate)\n",
    "        validKeys.append( keys[i] ) \n",
    "        labels.append(0)\n",
    "        lowCount = lowCount + 1\n",
    "    if rate > q3-eps:\n",
    "        #highRates.append(rate)\n",
    "        validKeys.append( keys[i] ) \n",
    "        labels.append(1)\n",
    "        highCount = highCount + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size:  448\n",
      "percentage:  29.78723404255319\n",
      "number of Cities with High HIV rates: 219 (0.489) Low HIV rates: 229 (0.511)\n"
     ]
    }
   ],
   "source": [
    "print ( \"sample size: \", len(validKeys))\n",
    "print ( \"percentage: \", len(validKeys)/len(newrates)*100 )\n",
    "print ( \"number of Cities with High HIV rates: {0} ({2:.3f}) Low HIV rates: {1} ({3:.3f})\".format(highCount, lowCount, highCount/len(validKeys), lowCount/len(validKeys)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tfidf(docID, wordID, tf, idf):\n",
    "    tf_0 = 0.5 +  tf[docID].get(wordID, 0)\n",
    "    idf_0 = math.log( 1 + N/len(idf[wordID]))\n",
    "    return (tf_0 * idf_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "random.seed(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Xtrain = csr_matrix ( (len(trainIndices) , VocabSize), dtype=float )\n",
    "#Xtest = csr_matrix (  (len(testIndices) , VocabSize), dtype=float )\n",
    "Ytrain = []\n",
    "Ytest = []\n",
    "#New generate matricies\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for i, docID in enumerate(validKeys):\n",
    "    for wordID in tf[docID]:\n",
    "        row.append(i)\n",
    "        col.append(wordID)\n",
    "        data.append(tfidf(docID, wordID, tf,idf) ) \n",
    "        \n",
    "Ytrain = labels\n",
    "Xtrain = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#sample data\n",
    "p = 1.00 #percentage of sampled data in\n",
    "Ytrain = []\n",
    "Ytest = []\n",
    "n = len(validKeys)\n",
    "validKeys0 = validKeys[:(int)(n/2)]\n",
    "validKeys1 = validKeys[(int)(n/2):]\n",
    "\n",
    "#need to keep it balanced\n",
    "dataSize = len(validKeys0)\n",
    "trainIndices0 = random.sample ( validKeys0, (int) (p*(dataSize)) ) #indicies start at 0\n",
    "testIndices0 = set(validKeys0) - set(trainIndices0)\n",
    "\n",
    "dataSize = len(validKeys1)\n",
    "trainIndices1 = random.sample ( validKeys1, (int) (p*(dataSize)) ) #indicies start at 1\n",
    "testIndices1 = set(validKeys1) - set(trainIndices1)\n",
    "\n",
    "testIndices = testIndices0.union(testIndices1)\n",
    "trainIndices = trainIndices0 + trainIndices1\n",
    "\n",
    "#Old generate matricies\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for i, docID in enumerate(trainIndices):\n",
    "    for wordID in tf[docID]:\n",
    "        row.append(i)\n",
    "        col.append(wordID)\n",
    "        data.append(tfidf(docID, wordID, tf,idf) ) \n",
    "    # uncomment to use regression\n",
    "    # Ytrain.append(locRates[docID]) \n",
    "    # used for classification\n",
    "    Ytrain.append (labels[ locRates[docID] ])\n",
    "Xtrain = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#generate matricies (testing Data) Not used since we are doing cross-validation instead.\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "for i, docID in enumerate(testIndices):\n",
    "    for wordID in tf[docID]:\n",
    "        row.append(i)\n",
    "        col.append(wordID)\n",
    "        data.append(tfidf(docID, wordID, tf,idf) ) \n",
    "        # Xtrain[i, wordID] = tfidf(docID, wordID, tf,idf)\n",
    "    #Ytest.append(locRates[docID])\n",
    "    Ytest.append (labels[ locRates[docID] ])\n",
    "Xtest = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(testIndices),VocabSize), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150 0\n",
      "(300, 184183) (60, 184183)\n"
     ]
    }
   ],
   "source": [
    "print (sum(Ytrain), sum(Ytest))\n",
    "print (Xtrain.shape, Xtest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 18 12 7\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               23              18\n",
      "predicted label lowHIVRates :                7              12\n",
      "[fold 0], accuracy: 0.58333, precision: 0.56098, recall: 0.76667\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "clf = MultinomialNB()\n",
    "Y_hat = clf.fit(Xtrain, Ytrain).predict(Xtest)\n",
    "cm = conf_mat(Y_hat, Ytest) \n",
    "print (\"confusion matrix:\")\n",
    "print (\"                             true label highHIVRates, true label lowHIVRates  \")\n",
    "print (\"predicted label highHIVRates:  {0:{width}d} {1:{width}d}\".format(cm[0], cm[1],width=15))\n",
    "print (\"predicted label lowHIVRates :  {0:{width}d} {1:{width}d}\".format(cm[3], cm[2],width=15))\n",
    "print (\"[fold {0}], accuracy: {1:.5f}, precision: {2:.5f}, recall: {3:.5f}\".\n",
    "      format(k, (cm[0]+cm[2])/(len(Y_hat)), cm[0]/(cm[0]+cm[1]), cm[0]/(cm[0]+cm[3])  ) )\n",
    "\n",
    "#acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "#prec = prec + cm[0]/(cm[0]+cm[1])\n",
    "#recal = recal + cm[0]/(cm[0]+cm[3])\n",
    "print (\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 0, 0, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1]\n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n"
     ]
    }
   ],
   "source": [
    "print (Ytest)\n",
    "print (Y_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 184183)\n",
      "184183\n"
     ]
    }
   ],
   "source": [
    "# standardization? needed for training?\n",
    "ss = [0]*VocabSize\n",
    "Mu = Xtrain.sum(0)/N #sum over all rows\n",
    "#print (Mu.shape)\n",
    "#print (VocabSize)\n",
    "count = 0\n",
    "for i,j in zip(row, col):\n",
    "    ss[j] = ss[j] + ((data[count] - Mu[0,j]) ** 2)\n",
    "    count = count + 1\n",
    "\n",
    "#print (len(data))\n",
    "#print(Mu[0,:])\n",
    "Sigma = []\n",
    "for item in ss:\n",
    "    Sigma.append( math.sqrt(item/N) )\n",
    "\n",
    "xvals = []\n",
    "count = 0\n",
    "for i,j in zip(row,col):\n",
    "    xvals.append((data[count] - Mu[0, j])/Sigma[j])\n",
    "    count = count + 1\n",
    "\n",
    "X_s = csr_matrix ( (np.array(xvals),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize), dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def conf_mat(Y_hat, Y):\n",
    "    tp = fp = tn = fn = 0\n",
    "    for i,j in zip(Y_hat, Y):\n",
    "        if i == 1:\n",
    "            if i == j:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        elif i == 0:\n",
    "            if i == j:\n",
    "                tn = tn + 1\n",
    "            else: \n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            print (\" j should only be 0 or 1, however\", j , \"was encountered.\")\n",
    "    print (tp, fp, tn, fn)\n",
    "    return [tp, fp, tn, fn]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22 15 15 8\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               22              15\n",
      "predicted label lowHIVRates :                8              15\n",
      "[fold 0], accuracy: 0.61667, precision: 0.59459, recall: 0.73333\n",
      "\n",
      "20 21 9 10\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               20              21\n",
      "predicted label lowHIVRates :               10               9\n",
      "[fold 1], accuracy: 0.48333, precision: 0.48780, recall: 0.66667\n",
      "\n",
      "23 11 19 7\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               23              11\n",
      "predicted label lowHIVRates :                7              19\n",
      "[fold 2], accuracy: 0.70000, precision: 0.67647, recall: 0.76667\n",
      "\n",
      "21 14 16 9\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               21              14\n",
      "predicted label lowHIVRates :                9              16\n",
      "[fold 3], accuracy: 0.61667, precision: 0.60000, recall: 0.70000\n",
      "\n",
      "26 21 9 4\n",
      "confusion matrix:\n",
      "                             true label highHIVRates, true label lowHIVRates  \n",
      "predicted label highHIVRates:               26              21\n",
      "predicted label lowHIVRates :                4               9\n",
      "[fold 4], accuracy: 0.58333, precision: 0.55319, recall: 0.86667\n",
      "\n",
      "average acc: 0.60000, average precision: 0.58241, average recall: 0.74667\n"
     ]
    }
   ],
   "source": [
    "#using naive Bayes for Classification\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "#from operator import add;\n",
    "clf = MultinomialNB()\n",
    "y = np.array(Ytrain)\n",
    "K = 5\n",
    "acc = 0\n",
    "prec = 0\n",
    "recal = 0\n",
    "k_fold = cross_validation.StratifiedKFold(Ytrain, n_folds=K,shuffle=True)\n",
    "\n",
    "for k, (train, test) in enumerate(k_fold):\n",
    "    Y_hat = clf.fit(Xtrain[train], y[train]).predict(Xtrain[test])\n",
    "    cm = conf_mat(Y_hat, y[test]) \n",
    "    print (\"confusion matrix:\")\n",
    "    print (\"                             true label highHIVRates, true label lowHIVRates  \")\n",
    "    print (\"predicted label highHIVRates:  {0:{width}d} {1:{width}d}\".format(cm[0], cm[1],width=15))\n",
    "    print (\"predicted label lowHIVRates :  {0:{width}d} {1:{width}d}\".format(cm[3], cm[2],width=15))\n",
    "    print (\"[fold {0}], accuracy: {1:.5f}, precision: {2:.5f}, recall: {3:.5f}\".\n",
    "          format(k, (cm[0]+cm[2])/(len(Y_hat)), cm[0]/(cm[0]+cm[1]), cm[0]/(cm[0]+cm[3])  ) )\n",
    "    acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "    prec = prec + cm[0]/(cm[0]+cm[1])\n",
    "    recal = recal + cm[0]/(cm[0]+cm[3])\n",
    "    print (\"\")\n",
    "\n",
    "print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc/K, prec/K, recal/K))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print ((int) (len(Ytrain)/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.LinearRegression()\n",
    "clf.fit (Xtrain, Ytrain) \n",
    "clf.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.092807781717191729"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1.0 is best possible score, lower is worse\n",
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.12343089742761792"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = linear_model.Ridge()\n",
    "clf.fit (Xtrain, Ytrain) \n",
    "clf.coef_\n",
    "#1.0 is best possible score, lower is worse\n",
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clf = linear_model.ElasticNet()\n",
    "clf.fit (Xtrain, Ytrain) \n",
    "clf.coef_\n",
    "#1.0 is best possible score, lower is worse\n",
    "clf.score(Xtest, Ytest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never give in in nothing , great or small , large or pettyexcept to convictions of honor and good sense . - Winston Churchill #quotes #quote\n",
      "R V P P N , A & A , A & N P N P N & A N , , ^ ^ # #\n",
      "26 26\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Debug\n",
    "lines = []\n",
    "with open( path2Data + 'Hawaii_HI_199.tsv', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "\n",
    "t0 = lines[199].strip().split('\\t')[0]\n",
    "t1 = lines[199].strip().split('\\t')[1]\n",
    "print (t0.strip())\n",
    "print (t1)\n",
    "print( len(t0.split()), len(t1.split()))\n",
    "for line in lines:\n",
    "    ll = line.split('\\t')\n",
    "    tweet = ll[0]\n",
    "    tweet_pos = ll[1]\n",
    "    \n",
    "    cleanTweet(tweet, tweet_pos)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
