{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "import nltk\n",
    "from statistics import *\n",
    "from sklearn import linear_model\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "#using naive Bayes for Classification\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import chi2\n",
    "#from operator import add;\n",
    "seed=1003\n",
    "random.seed(seed)\n",
    "path2Data=\"cities/\"\n",
    "stopwords = nltk.corpus.stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the POS tags of the tweets in path2Data\n",
    "# Since numbers and such give many unique ''words'' we can specify which to replace\n",
    "# currently replacing '#'-hashtags, '@'-usernames, 'U'-url links, 'E'-emoticons, '$'- numeral , ','-punctuations, 'G'-unknown tag\n",
    "# we are removing some formating symbols eg. \":\" which is tagged to '~'\n",
    "\n",
    "replaceables = ['#', '@', 'U', 'E', '$', ',', 'G']\n",
    "#replaceables = []\n",
    "removables = ['~']\n",
    "\n",
    "def cleanTweet(tweet, tweet_pos):\n",
    "    tweet_l = tweet.split()\n",
    "    tweet_pos_l = tweet_pos.split()\n",
    "\n",
    "    if len(tweet_l) != len(tweet_pos_l):\n",
    "        for i, item in enumerate(tweet_l):\n",
    "            print (tweet_l[i], ',' , tweet_pos_l[i])\n",
    "        \n",
    "    clean_tweet = []\n",
    "    for i, item in enumerate(tweet_l):\n",
    "        #print (item)\n",
    "        #print (tweet_pos_l[i])\n",
    "        if tweet_pos_l[i] in replaceables:\n",
    "            clean_tweet.append(tweet_pos_l[i])\n",
    "        elif tweet_pos_l[i] in removables:\n",
    "            None\n",
    "        else:\n",
    "            clean_tweet.append(item.lower())\n",
    "    \n",
    "    #print (clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "# Version 2: generating Valid Keys for corresponding to the top/bottom HIV rates\n",
    "#sample size is 2 (p*N), p*N for lower and p*N for upper\n",
    "def sampleItems(locRates, p):\n",
    "    items = []\n",
    "    for key in locRates:\n",
    "        items.append((key, locRates[key]))\n",
    "\n",
    "    sorted_locRates = sorted(items, key=lambda student: student[1]) \n",
    "    total_items = len(items)\n",
    "    sampleSize = (int) (p*total_items)\n",
    "    #print (\"sample size: \", sampleSize)\n",
    "    \n",
    "    ret = []\n",
    "    lab = {}\n",
    "    for i, item in enumerate(sorted_locRates):\n",
    "        if i < sampleSize:\n",
    "            ret.append(item[0])\n",
    "            lab[item[1]] = 0\n",
    "            \n",
    "        if i >= total_items - sampleSize:\n",
    "            ret.append(item[0])\n",
    "            #lab.append(1)\n",
    "            lab[item[1]] = 1\n",
    "    \n",
    "    #print (\"samped items size: \", len(ret))\n",
    "    return (ret,lab)\n",
    "    \n",
    "\n",
    "def tfidf(docID, wordID, tf, idf, N):\n",
    "    tf_0 = 0.5 +  tf[docID].get(wordID, 0)\n",
    "    idf_0 = math.log( 1 + N/len(idf[wordID]))\n",
    "    #idf_0 = 1\n",
    "    return (tf_0 * idf_0)\n",
    "\n",
    "def conf_mat(Y_hat, Y):\n",
    "    tp = fp = tn = fn = 0\n",
    "    for i,j in zip(Y_hat, Y):\n",
    "        if i == 1:\n",
    "            if i == j:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        elif i == 0:\n",
    "            if i == j:\n",
    "                tn = tn + 1\n",
    "            else: \n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            print (\" j should only be 0 or 1, however\", j , \"was encountered.\")\n",
    "    #print (tp, fp, tn, fn)\n",
    "    return [tp, fp, tn, fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "filenum = 0\n",
    "Vcount=0\n",
    "VVcount=0\n",
    "Vinv={}          # index to word map\n",
    "V={}             # word to index\n",
    "VV={}\n",
    "VVinv={}\n",
    "idf={}           # forwardIndex\n",
    "tf={}            # (word, numberOfWords)\n",
    "bitf={}\n",
    "locRates={}      # HIV rates based on locations\n",
    "\n",
    "N = 0\n",
    "for file in glob.glob(path2Data+'*.tsv'):\n",
    "    filenum = filenum + 1  #serves as an index for the file name\n",
    "    prefix = file.split('.')[0]\n",
    "    locRates[filenum] = int(prefix.split('_')[-1])\n",
    "\n",
    "    lines = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    #DEBUG\n",
    "    #print (file + \" file num: \" + str(filenum) + \" num tweets: \" + str(len(lines)) )\n",
    "\n",
    "    unique_words = set([])\n",
    "    for line in lines:\n",
    "        ll = line.split('\\t')\n",
    "        tweet = ll[0].strip()\n",
    "        tweet_pos = ll[1].strip()\n",
    "\n",
    "        prevWord = \"<s>\"\n",
    "        #for word in cleanTweet(tweet, tweet_pos):\n",
    "        for word in (tweet+tweet_pos).replace(removables[0], '').:\n",
    "            if word not in V:\n",
    "                V[word]= Vcount\n",
    "                Vinv[Vcount]=word\n",
    "                Vcount = Vcount + 1\n",
    "\n",
    "            #bigram \n",
    "#             if (prevWord,word) not in VV:\n",
    "#                 VV[(prevWord, word)] = VVcount\n",
    "#                 VVinv[VVcount] = (prevWord,word)\n",
    "#                 VVcount = VVcount + 1\n",
    "\n",
    "            if V[word] not in idf:\n",
    "                idf[ V[word] ] = []\n",
    "\n",
    "            if filenum not in tf:\n",
    "                tf[filenum] = {}\n",
    "\n",
    "            freq = tf[filenum].get(V[word], 0)\n",
    "            tf[filenum][ V[word] ] = freq + 1\n",
    "\n",
    "            if word not in unique_words:\n",
    "                idf[ V[word] ].append(filenum)\n",
    "                unique_words.add(word)\n",
    "\n",
    "            #bigram\n",
    "#             if filenum not in bitf:\n",
    "#                 bitf[filenum] = {}\n",
    "\n",
    "#             bitf[filenum][VV[(prevWord, word)]] = bitf[filenum].get(VV[(prevWord,word)], 0) + 1\n",
    "#             prevWord = word\n",
    "\n",
    "N = filenum\n",
    "VocabSize=len(V.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "frequency = {}\n",
    "for file in glob.glob(path2Data+'*.tsv'):\n",
    "    filenum = filenum + 1  #serves as an index for the file name\n",
    "    prefix = file.split('.')[0]\n",
    "    locRates[filenum] = int(prefix.split('_')[-1])\n",
    "\n",
    "    lines = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    unique_words = set([])\n",
    "    for line in lines:\n",
    "        ll = line.split('\\t')\n",
    "        tweet = ll[0].strip()\n",
    "        tweet_pos = ll[1].strip()\n",
    "\n",
    "        for word in cleanTweet(tweet, tweet_pos):\n",
    "            if word not in unique_words:\n",
    "                frequency[word] = 1 + frequency.get(word,0)\n",
    "                unique_words.add(word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fv = list(frequency.values())\n",
    "x = []\n",
    "y = []\n",
    "for item in set(fv):\n",
    "    x.append(item)\n",
    "    y.append(fv.count(item))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 418\n",
      "1\n",
      "1498\n"
     ]
    }
   ],
   "source": [
    "print(median(y), median(x))\n",
    "print(median(fv))\n",
    "print(max(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(np.log10(x), np.log10(y) )\n",
    "plt.xlabel('frequency of occurence in tweets')\n",
    "plt.ylabel('count of words')\n",
    "plt.title('frequency vs counts')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47712125471966244"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.log10(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some statistics about the data\n",
    "print (\"vocabSize: \", VocabSize)\n",
    "print (\"docSize: \", N)\n",
    "print (\"labels: \", len(locRates))\n",
    "mu = mean(locRates.values())\n",
    "print (\"mean: \",  mu )\n",
    "med = median(locRates.values())\n",
    "print (\"median: \",  median(locRates.values()) )\n",
    "print (\"max: \", max(locRates.values()) )\n",
    "print (\"min: \", min(locRates.values()) )\n",
    "sigma = stdev(locRates.values(), mu)\n",
    "print (\"standard diviation: \", sigma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "locRates.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Regression\n",
    "#NB classification \n",
    "x_cord = []\n",
    "\n",
    "#sample top/bottom rates\n",
    "\n",
    "sss = [0.25]\n",
    "#sss= np.arange(0.05, 0.48, 0.02)\n",
    "topPercent = 0.25\n",
    "(validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "data_tf = []\n",
    "Yreg = []\n",
    "print (len(validKeys))\n",
    "for i, docID in enumerate(validKeys):\n",
    "    for wordID in tf[docID]:\n",
    "        row.append(i)\n",
    "        col.append(wordID)\n",
    "        data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "        data_tf.append(tf[docID][wordID])\n",
    "#     # uncomment to use regression\n",
    "    Yreg.append(locRates[docID]) \n",
    "    # used for classification\n",
    "X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=np.dtype('d'))\n",
    "#X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "print (X[row[0], col[0]])\n",
    "print ( Yreg[0] )\n",
    "#bigram\n",
    "#X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize+len(VV.keys())), dtype=float)\n",
    "print (\"shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#clf = MultinomialNB()\n",
    "clf_v2 = linear_model.ElasticNetCV(l1_ratio=[0.75, 0.80, 0.85, 0.90, 0.95], n_jobs=3, cv=5, alphas=np.array([0.1, 1.0, 10, 100, 1000, 10000, 100000]))\n",
    "#print (clf_v2.get_params())\n",
    "clf_v2.fit(X, Yreg)\n",
    "#alphas_enet, coefs_enet, _ = linear_model.enet_path(  X, np.array(Yreg, dtype=np.dtype('d')), eps=0.005, l1_ratio=0.8, fit_intercept=False)\n",
    "\n",
    "#clf = linear_model.Lasso(alpha=0.1)\n",
    "#clf = linear_model.SGDClassifier()\n",
    "#     train_errors = list()\n",
    "#     test_errors = list()\n",
    "#     alphas = np.logspace(-5, 1, 10)\n",
    "#     y_ridge = np.array(Yreg)\n",
    "#     K = 5\n",
    "#     print (alphas)\n",
    "#     k_fold = cross_validation.KFold(len(y_ridge), n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "#     #clf.set_params(alpha=alphas[0])\n",
    "#     #clf.fit(X[train], y_ridge[train])\n",
    "#     #for i, alpha in enumerate(alphas):\n",
    "#     #    print (i, \":\", alpha)\n",
    "#     #    clf.set_params(alpha=alpha)\n",
    "#     #    tr_err = 0\n",
    "#     #    ts_err = 0\n",
    "#     for train, test in k_fold:\n",
    "#         print(\"training\")\n",
    "#         clf.fit(X[train], y_ridge[train])\n",
    "#             #tr_err = tr_err + clf.score(X[train], y_ridge[train])\n",
    "#             #ts_err = ts_err + clf.score(X[test], y_ridge[test])\n",
    "#             #clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "#     train_errors.append(tr_err/K)\n",
    "#     test_errors.append(ts_err/K)\n",
    "    \n",
    "#     #print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc/K, prec/K, recal/K))\n",
    "#     #print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc_tf/K, prec_tf/K, recal_tf/K))\n",
    "#     x_cord.append(topPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas_ = clf_v2.alphas_\n",
    "print(alphas_)\n",
    "print (len(alphas_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (clf_v2.alpha_, clf_v2.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open( \"regression/temp.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas_ = [  1.00000000e+05 ,  1.00000000e+04 ,  1.00000000e+03   ,1.00000000e+02, 1.00000000e+01 ,  1.00000000e+00 ,  1.00000000e-01]\n",
    "mse_ =[]\n",
    "for line in lines:\n",
    "    temporino = [ float(item) for item in line.split()]\n",
    "    mse_.append(sum(temporino)/len(temporino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(mse_[:7])\n",
    "print(mse_[7:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open (\"regression/elastic_net_cv10_l1r90_alpha.txt\", 'w') as f:\n",
    "    for alpha in alphas_:\n",
    "        f.write( \"\" + str(alpha) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_path_var = clf_v2.mse_path_\n",
    "mse1=[]\n",
    "mse2=[]\n",
    "mse3=[]\n",
    "mse_avg=[]\n",
    "#print (mse_path_)\n",
    "with open (\"regression/elastic_net_cv10_l1r90_var_mse.txt\", 'w') as f:\n",
    "    for c in mse_path_var:\n",
    "        f.write( str(c) + \"\\n\") \n",
    "        mse_avg.append( sum(c)/ len(c))\n",
    "#         f.write( str(cv1) + \",\" + str(cv2) + \",\"+str(cv3) + \"\\n\")\n",
    "#         mse1.append(cv1)\n",
    "#         mse2.append(cv2)\n",
    "#         mse3.append(cv3)\n",
    "#         mse_avg.append((cv1+cv2+cv3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_ = clf_v2.coef_\n",
    "len(coef_)\n",
    "with open (\"regression/elastic_net_cv10_l1r90_coef.txt\", 'w') as f:\n",
    "    for c in coef_:\n",
    "        f.write( \"\" + str(c) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Elastic-Net MSE l1-ratios [0.75, 0.8, 0.85, 0.9, 0.95]')\n",
    "# plt.subplot(321)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[0:7]), ':' , label='0.75')\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# plt.subplot(322)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[7:14]), '--' , label='0.80')\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.80')\n",
    "# plt.subplot(323)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[14:21]), 'o-', label='0.85' )\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.85')\n",
    "# plt.subplot(324)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[21:28]), '.-', label='0.90' )\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.90')\n",
    "# plt.subplot(325)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[28:]), label='0.95' )\n",
    "\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Log(MSE)')\n",
    "#plt.title('Elastic-Net MSE l1-ratio  0.95')\n",
    "plt.legend()\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse2) )\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse3) )\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_avg = [ sum(mse)/len(mse) for mse in clf_v2.mse_path_[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_avg) )\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Log(mse)')\n",
    "plt.title('Elastic-Net Parameter selection via log_10(MSE)')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "plt.plot(range(len(coef_)), coef_ )\n",
    "plt.xlabel('Feature Number')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Features')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_list = []\n",
    "for i, c in enumerate(coef_):\n",
    "    c_list.append((i , c))\n",
    "\n",
    "sorted_c = sorted(c_list, key=lambda student: student[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open (\"regression/elastic_net_cv3_l1r85_top100Words.txt\", 'w') as f:\n",
    "    for i in sorted_c[:50]:\n",
    "        f.write( \"{0:.5f}\".format( i[1]) + \"\\t\" +  Vinv [i[0]] + \"\\n\")    \n",
    "    for i in sorted_c[-50:]:\n",
    "        f.write ( \"{0:.5f}\".format( i[1])+ \"\\t\" +  Vinv [i[0]] + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n",
    "\n",
    "# normalize data as done by Lars to allow for comparison\n",
    "X /= np.sqrt(np.sum(X ** 2, axis=0))\n",
    "\n",
    "print (X.shape)\n",
    "\n",
    "# Compute paths\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = LassoCV(cv=20).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#NB classification \n",
    "x_cord = []\n",
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "tf_accuracies = []\n",
    "tf_precision = []\n",
    "tf_recall = []\n",
    "#sample top/bottom rates\n",
    "for curr in np.arange(0.05, 0.48, 0.02):\n",
    "    topPercent = curr\n",
    "    (validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "    print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    data_tf = []\n",
    "    Yclass = []\n",
    "    Yreg = []\n",
    "\n",
    "    for i, docID in enumerate(validKeys):\n",
    "        for wordID in tf[docID]:\n",
    "            row.append(i)\n",
    "            col.append(wordID)\n",
    "            data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "            data_tf.append(tf[docID][wordID])\n",
    "    #     # bigram\n",
    "    #     for bigramID in bitf[docID]:\n",
    "    #         row.append(i)\n",
    "    #         col.append(VocabSize + bigramID)\n",
    "    #         data.append(bitf[docID][bigramID])\n",
    "\n",
    "    #     # uncomment to use regression\n",
    "        Yreg.append(locRates[docID]) \n",
    "        # used for classification\n",
    "        Yclass.append (labels[ locRates[docID] ])\n",
    "    X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    #bigram\n",
    "    #X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize+len(VV.keys())), dtype=float)\n",
    "    #print (X.shape)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    #clf = linear_model.SGDClassifier()\n",
    "    y = np.array(Yclass)\n",
    "    y_ridge = np.array(Yreg)\n",
    "    K = 5\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    recal = 0\n",
    "    acc_tf=0\n",
    "    prec_tf=0\n",
    "    recal_tf=0\n",
    "    k_fold = cross_validation.StratifiedKFold(Yclass, n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "    avg_ela = 0\n",
    "    terms = []\n",
    "    tf_terms = []\n",
    "    for k, (train, test) in enumerate(k_fold):\n",
    "        Y_hat = clf.fit(X[train], y[train]).predict(X[test])\n",
    "        Y_tf_hat = clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "#         ch2 = feature_selection.SelectKBest(feature_selection.chi2, k=150)\n",
    "#         X_new = ch2.fit_transform(X, y)\n",
    "#         #X_test = ch2.transform(X[test])\n",
    "#         Y_hat = clf.fit(X_new[train], y[train]).predict(X_new[test])\n",
    "#         if (k == 0 ):\n",
    "#             terms = [ Vinv[feature] for feature in ch2.get_support(indices=True) ] \n",
    "#             print (terms)\n",
    "            \n",
    "        #for feature in ch2.get_support(indices=True):\n",
    "        #\tprint ( Vinv[feature] )\n",
    "        \n",
    "#         X_tf_new = ch2.fit_transform(X_tf,y)\n",
    "#         Y_tf_hat = clf.fit(X_tf_new[train], y[train]).predict(X_tf_new[test])\n",
    "#         if (k == 0 ):\n",
    "#             terms_tf = [ Vinv[feature] for feature in ch2.get_support(indices=True) ] \n",
    "#             print (terms_tf)\n",
    "        \n",
    "        cm = conf_mat(Y_hat, y[test])\n",
    "        cm_tf = conf_mat(Y_tf_hat, y[test])\n",
    "        \n",
    "        acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "        prec = prec + cm[0]/max(cm[0]+cm[1],1)\n",
    "        recal = recal + cm[0]/max(cm[0]+cm[3],1)\n",
    "        \n",
    "        acc_tf = (cm_tf[0]+cm_tf[2])/(len(Y_tf_hat)) + acc_tf\n",
    "        prec_tf = prec_tf + cm_tf[0]/max(cm_tf[0]+cm_tf[1],1)\n",
    "        recal_tf = recal_tf + cm_tf[0]/max(cm_tf[0]+cm_tf[3],1)\n",
    "\n",
    "    print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc/K, prec/K, recal/K))\n",
    "    print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc_tf/K, prec_tf/K, recal_tf/K))\n",
    "    x_cord.append(topPercent)\n",
    "    accuracies.append(acc/K)\n",
    "    precision.append(prec/K)\n",
    "    recall.append(recal/K)\n",
    "    tf_accuracies.append(acc_tf/K)\n",
    "    tf_precision.append(prec_tf/K)\n",
    "    tf_recall.append(recal_tf/K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fe_accuracies = np.empty([22,40])\n",
    "fe_tf_accuracies = np.empty([22,40])\n",
    "#sample top/bottom rates\n",
    "for ia, curr in enumerate(np.arange(0.05, 0.48, 0.02)):\n",
    "    topPercent = curr\n",
    "    (validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "    print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    data_tf = []\n",
    "    Yclass = []\n",
    "    Yreg = []\n",
    "\n",
    "    for i, docID in enumerate(validKeys):\n",
    "        for wordID in tf[docID]:\n",
    "            row.append(i)\n",
    "            col.append(wordID)\n",
    "            data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "            data_tf.append(tf[docID][wordID])\n",
    "\n",
    "        #     # uncomment to use regression\n",
    "        Yreg.append(locRates[docID]) \n",
    "        # used for classification\n",
    "        Yclass.append (labels[ locRates[docID] ])\n",
    "    X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    \n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    #clf = linear_model.SGDClassifier()\n",
    "    y = np.array(Yclass)\n",
    "    #y_ridge = np.array(Yreg)\n",
    "    K = 5\n",
    "    k_fold = cross_validation.StratifiedKFold(Yclass, n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "    #avg_ela = 0\n",
    "    #terms = []\n",
    "    #tf_terms = []\n",
    "    for ja, topFE in enumerate(range(100, 5000, 100)):\n",
    "        acc = 0\n",
    "        acc_tf=0\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            #Y_hat = clf.fit(X[train], y[train]).predict(X[test])\n",
    "            #Y_tf_hat = clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "            ch2 = feature_selection.SelectKBest(feature_selection.chi2, k=topFE)\n",
    "            X_new = ch2.fit_transform(X, y)\n",
    "             #X_test = ch2.transform(X[test])\n",
    "            Y_hat = clf.fit(X_new[train], y[train]).predict(X_new[test])\n",
    "    \n",
    "            X_tf_new = ch2.fit_transform(X_tf,y)\n",
    "            Y_tf_hat = clf.fit(X_tf_new[train], y[train]).predict(X_tf_new[test])\n",
    "    \n",
    "            cm = conf_mat(Y_hat, y[test])\n",
    "            cm_tf = conf_mat(Y_tf_hat, y[test])\n",
    "\n",
    "            acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "            acc_tf = (cm_tf[0]+cm_tf[2])/(len(Y_tf_hat)) + acc_tf\n",
    "        \n",
    "        #x_cord.append(topPercent)\n",
    "        fe_accuracies[ia][ja] = acc/K\n",
    "#         precision.append(prec/K)\n",
    "#         recall.append(recal/K)\n",
    "        fe_tf_accuracies[ia][ja]=acc_tf/K\n",
    "#         tf_precision.append(prec_tf/K)\n",
    "#         tf_recall.append(recal_tf/K)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = fe_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttt = temp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ttt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for ia, curr in enumerate(np.arange(0.05, 0.48, 0.02)):\n",
    "    for ja, topFE in enumerate(range(100, 4100, 100)):\n",
    "        xs.append(((int)(curr*N))*2)\n",
    "        ys.append(topFE)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "#mpl.rcParams['title.fontsize'] = 10\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot_trisurf(xs, ys, ttt, cmap=cm.jet, linewidth=0.2)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Sample size')\n",
    "ax.set_ylabel('top K best features')\n",
    "ax.set_zlabel('Accuracy')\n",
    "ax.set_title('Naive Bayes Classification 5-fold Cross Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open ('tables/acc_chi2_sampleSize.txt' , 'w') as f:\n",
    "    f.write(\"sampleSize, K, accuracy  \\n\")\n",
    "    for i in range(len(xs)):\n",
    "        #print (xs[i], ys[i], ttt[i])\n",
    "        f.write( \"{0:10d}, {1:10d}, {2:.3f}\\n\".format(xs[i], ys[i], ttt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
    "z = np.linspace(-2, 2, 100)\n",
    "r = z**2 + 1\n",
    "x = r * np.sin(theta)\n",
    "y = r * np.cos(theta)\n",
    "ax.plot(x, y, z, label='parametric curve')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fe_accuracies[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (terms_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies_150 = accuracies\n",
    "accuracies_tf_150 = tf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_accuracies_no_filter = accuracies\n",
    "tf_accu_no_filter = tf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, accuracies, label=\"tfidf+POS\")\n",
    "plt.plot(x_cord, tf_accuracies, label=\"tf+POS\")\n",
    "#plt.plot(x_cord, accuracies_150, label=\"tfidf + POS\")\n",
    "#plt.plot(x_cord, accuracies_tf_150, label=\"tf + POS\")\n",
    "plt.plot(x_cord, idf_accuracies_no_filter, label=\"tfidf\")\n",
    "plt.plot(x_cord, tf_accu_no_filter, label=\"tf\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, precision, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, tf_precision, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, recall, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, tf_recall, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = []\n",
    "f1_tf = []\n",
    "for p, r in zip(precision, recall):\n",
    "    f1.append( 2*(p*r)/(p+r))\n",
    "    \n",
    "for p, r in zip(tf_precision, tf_recall):\n",
    "    f1_tf.append( 2*(p*r)/(p+r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('tables/multinomail_accuracy.txt', 'w') as f:\n",
    "    f.write(\"sample size, tfidf+POS, tf+POS, tfidf, tf  \\n\")\n",
    "    for i in range(len(x_cord)):\n",
    "        f.write( \"{0:10d}, {1:0.3f}, {2:.3f}, {3:.3f}, {4:.3f}\\n\".format((int)(x_cord[i]*N*2), accuracies[i], tf_accuracies[i], idf_accuracies_no_filter[i], tf_accu_no_filter[i]))\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in accuracies) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in tf_accuracies) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in idf_accuracies_no_filter) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in tf_accu_no_filter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save for experimentation with the size of feature extraction\n",
    "accuracy_no_fe = accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "[i for i in range(len(x_cord))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(x_cord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('tables/multinomail_accuracy.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "sampleSize = []\n",
    "tfidfPOS = []\n",
    "tfPOS = []\n",
    "tfidf = []\n",
    "tf = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i > 0:\n",
    "        temp = line.strip().split(',')\n",
    "        sampleSize.append(int(temp[0]))\n",
    "        tfidfPOS.append(float(temp[1]))\n",
    "        tfPOS.append(float(temp[2]))\n",
    "        tfidf.append(float(temp[3]))\n",
    "        tf.append(float(temp[4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sampleSize, tfidfPOS, label=\"tfidf+POS\")\n",
    "plt.plot(sampleSize, tfPOS, label=\"tf+POS\")\n",
    "#plt.plot(x_cord, accuracies_150, label=\"tfidf + POS\")\n",
    "#plt.plot(x_cord, accuracies_tf_150, label=\"tf + POS\")\n",
    "plt.plot(sampleSize, tfidf, label=\"tfidf\")\n",
    "plt.plot(sampleSize, tf, label=\"tf\")\n",
    "plt.xlabel('total (training + testing) sample size ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classifier 5-fold Cross Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print (lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, f1, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, f1_tf, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
