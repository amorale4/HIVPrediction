{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "import math\n",
    "from statistics import *\n",
    "from sklearn import linear_model\n",
    "from scipy.sparse import csr_matrix\n",
    "import numpy as np\n",
    "import random\n",
    "#using naive Bayes for Classification\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import cross_validation\n",
    "from sklearn import feature_selection\n",
    "from sklearn.feature_selection import chi2\n",
    "#from operator import add;\n",
    "seed=1003\n",
    "random.seed(seed)\n",
    "path2Data=\"cities/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# reads the POS tags of the tweets in path2Data\n",
    "# Since numbers and such give many unique ''words'' we can specify which to replace\n",
    "# currently replacing '#'-hashtags, '@'-usernames, 'U'-url links, 'E'-emoticons, '$'- numeral , ','-punctuations, 'G'-unknown tag\n",
    "# we are removing some formating symbols eg. \":\" which is tagged to '~'\n",
    "\n",
    "replaceables = ['#', '@', 'U', 'E', '$', ',', 'G']\n",
    "#replaceables = []\n",
    "removables = ['~']\n",
    "\n",
    "def cleanTweet(tweet, tweet_pos):\n",
    "    tweet_l = tweet.split()\n",
    "    tweet_pos_l = tweet_pos.split()\n",
    "\n",
    "    if len(tweet_l) != len(tweet_pos_l):\n",
    "        for i, item in enumerate(tweet_l):\n",
    "            print (tweet_l[i], ',' , tweet_pos_l[i])\n",
    "        \n",
    "    clean_tweet = []\n",
    "    for i, item in enumerate(tweet_l):\n",
    "        #print (item)\n",
    "        #print (tweet_pos_l[i])\n",
    "        if tweet_pos_l[i] in replaceables:\n",
    "            clean_tweet.append(tweet_pos_l[i])\n",
    "        elif tweet_pos_l[i] in removables:\n",
    "            None\n",
    "        else:\n",
    "            clean_tweet.append(item.lower())\n",
    "    \n",
    "    #print (clean_tweet)\n",
    "    return clean_tweet\n",
    "\n",
    "# Version 2: generating Valid Keys for corresponding to the top/bottom HIV rates\n",
    "#sample size is 2 (p*N), p*N for lower and p*N for upper\n",
    "def sampleItems(locRates, p):\n",
    "    items = []\n",
    "    for key in locRates:\n",
    "        items.append((key, locRates[key]))\n",
    "\n",
    "    sorted_locRates = sorted(items, key=lambda student: student[1]) \n",
    "    total_items = len(items)\n",
    "    sampleSize = (int) (p*total_items)\n",
    "    #print (\"sample size: \", sampleSize)\n",
    "    \n",
    "    ret = []\n",
    "    lab = {}\n",
    "    for i, item in enumerate(sorted_locRates):\n",
    "        if i < sampleSize:\n",
    "            ret.append(item[0])\n",
    "            lab[item[1]] = 0\n",
    "            \n",
    "        if i >= total_items - sampleSize:\n",
    "            ret.append(item[0])\n",
    "            #lab.append(1)\n",
    "            lab[item[1]] = 1\n",
    "    \n",
    "    #print (\"samped items size: \", len(ret))\n",
    "    return (ret,lab)\n",
    "    \n",
    "\n",
    "def tfidf(docID, wordID, tf, idf, N):\n",
    "    tf_0 = 0.5 +  tf[docID].get(wordID, 0)\n",
    "    idf_0 = math.log( 1 + N/len(idf[wordID]))\n",
    "    #idf_0 = 1\n",
    "    return (tf_0 * idf_0)\n",
    "\n",
    "def conf_mat(Y_hat, Y):\n",
    "    tp = fp = tn = fn = 0\n",
    "    for i,j in zip(Y_hat, Y):\n",
    "        if i == 1:\n",
    "            if i == j:\n",
    "                tp = tp + 1\n",
    "            else:\n",
    "                fp = fp + 1\n",
    "        elif i == 0:\n",
    "            if i == j:\n",
    "                tn = tn + 1\n",
    "            else: \n",
    "                fn = fn + 1\n",
    "        else:\n",
    "            print (\" j should only be 0 or 1, however\", j , \"was encountered.\")\n",
    "    #print (tp, fp, tn, fn)\n",
    "    return [tp, fp, tn, fn]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "filenum = 0\n",
    "Vcount=0\n",
    "VVcount=0\n",
    "Vinv={}          # index to word map\n",
    "V={}             # word to index\n",
    "VV={}\n",
    "VVinv={}\n",
    "idf={}           # forwardIndex\n",
    "tf={}            # (word, numberOfWords)\n",
    "bitf={}\n",
    "locRates={}      # HIV rates based on locations\n",
    "\n",
    "N = 0\n",
    "for file in glob.glob(path2Data+'*.tsv'):\n",
    "    filenum = filenum + 1  #serves as an index for the file name\n",
    "    prefix = file.split('.')[0]\n",
    "    locRates[filenum] = int(prefix.split('_')[-1])\n",
    "\n",
    "    lines = []\n",
    "    with open(file, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "    #DEBUG\n",
    "    #print (file + \" file num: \" + str(filenum) + \" num tweets: \" + str(len(lines)) )\n",
    "\n",
    "    unique_words = set([])\n",
    "    for line in lines:\n",
    "        ll = line.split('\\t')\n",
    "        tweet = ll[0].strip()\n",
    "        tweet_pos = ll[1].strip()\n",
    "\n",
    "        prevWord = \"<s>\"\n",
    "        for word in cleanTweet(tweet, tweet_pos):\n",
    "            if word not in V:\n",
    "                V[word]= Vcount\n",
    "                Vinv[Vcount]=word\n",
    "                Vcount = Vcount + 1\n",
    "\n",
    "            #bigram \n",
    "#             if (prevWord,word) not in VV:\n",
    "#                 VV[(prevWord, word)] = VVcount\n",
    "#                 VVinv[VVcount] = (prevWord,word)\n",
    "#                 VVcount = VVcount + 1\n",
    "\n",
    "            if V[word] not in idf:\n",
    "                idf[ V[word] ] = []\n",
    "\n",
    "            if filenum not in tf:\n",
    "                tf[filenum] = {}\n",
    "\n",
    "            freq = tf[filenum].get(V[word], 0)\n",
    "            tf[filenum][ V[word] ] = freq + 1\n",
    "\n",
    "            if word not in unique_words:\n",
    "                idf[ V[word] ].append(filenum)\n",
    "                unique_words.add(word)\n",
    "\n",
    "            #bigram\n",
    "#             if filenum not in bitf:\n",
    "#                 bitf[filenum] = {}\n",
    "\n",
    "#             bitf[filenum][VV[(prevWord, word)]] = bitf[filenum].get(VV[(prevWord,word)], 0) + 1\n",
    "#             prevWord = word\n",
    "\n",
    "N = filenum\n",
    "VocabSize=len(V.keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabSize:  184183\n",
      "docSize:  1504\n",
      "labels:  1504\n",
      "mean:  133.54255319148936\n",
      "median:  82.5\n",
      "max:  2084\n",
      "min:  11\n",
      "standard diviation:  160.6118525618946\n"
     ]
    }
   ],
   "source": [
    "#some statistics about the data\n",
    "print (\"vocabSize: \", VocabSize)\n",
    "print (\"docSize: \", N)\n",
    "print (\"labels: \", len(locRates))\n",
    "mu = mean(locRates.values())\n",
    "print (\"mean: \",  mu )\n",
    "med = median(locRates.values())\n",
    "print (\"median: \",  median(locRates.values()) )\n",
    "print (\"max: \", max(locRates.values()) )\n",
    "print (\"min: \", min(locRates.values()) )\n",
    "sigma = stdev(locRates.values())\n",
    "print (\"standard diviation: \", sigma )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top sample size:  376  top:  0.25\n",
      "752\n",
      "3.40070866448\n",
      "11\n",
      "shape: (752, 184183)\n"
     ]
    }
   ],
   "source": [
    "#Regression\n",
    "#NB classification \n",
    "x_cord = []\n",
    "\n",
    "#sample top/bottom rates\n",
    "\n",
    "sss = [0.25]\n",
    "#sss= np.arange(0.05, 0.48, 0.02)\n",
    "topPercent = 0.25\n",
    "(validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "row = []\n",
    "col = []\n",
    "data = []\n",
    "data_tf = []\n",
    "Yreg = []\n",
    "print (len(validKeys))\n",
    "for i, docID in enumerate(validKeys):\n",
    "    for wordID in tf[docID]:\n",
    "        row.append(i)\n",
    "        col.append(wordID)\n",
    "        data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "        data_tf.append(tf[docID][wordID])\n",
    "#     # uncomment to use regression\n",
    "    Yreg.append(locRates[docID]) \n",
    "    # used for classification\n",
    "X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=np.dtype('d'))\n",
    "#X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "print (X[row[0], col[0]])\n",
    "print ( Yreg[0] )\n",
    "#bigram\n",
    "#X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize+len(VV.keys())), dtype=float)\n",
    "print (\"shape:\", X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/squirtle/local/anaconda3/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py:490: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n",
      "/home/squirtle/local/anaconda3/lib/python3.4/site-packages/sklearn/linear_model/coordinate_descent.py:490: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations\n",
      "  ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ElasticNetCV(alphas=array([  1.00000e-01,   1.00000e+00,   1.00000e+01,   1.00000e+02,\n",
       "         1.00000e+03,   1.00000e+04,   1.00000e+05]),\n",
       "       copy_X=True, cv=5, eps=0.001, fit_intercept=True,\n",
       "       l1_ratio=[0.75, 0.8, 0.85, 0.9, 0.95], max_iter=1000, n_alphas=100,\n",
       "       n_jobs=3, normalize=False, positive=False, precompute='auto',\n",
       "       tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#clf = MultinomialNB()\n",
    "clf_v2 = linear_model.ElasticNetCV(l1_ratio=[0.75, 0.80, 0.85, 0.90, 0.95], n_jobs=3, cv=5, alphas=np.array([0.1, 1.0, 10, 100, 1000, 10000, 100000]))\n",
    "#print (clf_v2.get_params())\n",
    "clf_v2.fit(X, Yreg)\n",
    "#alphas_enet, coefs_enet, _ = linear_model.enet_path(  X, np.array(Yreg, dtype=np.dtype('d')), eps=0.005, l1_ratio=0.8, fit_intercept=False)\n",
    "\n",
    "#clf = linear_model.Lasso(alpha=0.1)\n",
    "#clf = linear_model.SGDClassifier()\n",
    "#     train_errors = list()\n",
    "#     test_errors = list()\n",
    "#     alphas = np.logspace(-5, 1, 10)\n",
    "#     y_ridge = np.array(Yreg)\n",
    "#     K = 5\n",
    "#     print (alphas)\n",
    "#     k_fold = cross_validation.KFold(len(y_ridge), n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "#     #clf.set_params(alpha=alphas[0])\n",
    "#     #clf.fit(X[train], y_ridge[train])\n",
    "#     #for i, alpha in enumerate(alphas):\n",
    "#     #    print (i, \":\", alpha)\n",
    "#     #    clf.set_params(alpha=alpha)\n",
    "#     #    tr_err = 0\n",
    "#     #    ts_err = 0\n",
    "#     for train, test in k_fold:\n",
    "#         print(\"training\")\n",
    "#         clf.fit(X[train], y_ridge[train])\n",
    "#             #tr_err = tr_err + clf.score(X[train], y_ridge[train])\n",
    "#             #ts_err = ts_err + clf.score(X[test], y_ridge[test])\n",
    "#             #clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "#     train_errors.append(tr_err/K)\n",
    "#     test_errors.append(ts_err/K)\n",
    "    \n",
    "#     #print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc/K, prec/K, recal/K))\n",
    "#     #print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc_tf/K, prec_tf/K, recal_tf/K))\n",
    "#     x_cord.append(topPercent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  1.00000000e+05   1.00000000e+04   1.00000000e+03   1.00000000e+02\n",
      "   1.00000000e+01   1.00000000e+00   1.00000000e-01]\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "alphas_ = clf_v2.alphas_\n",
    "print(alphas_)\n",
    "print (len(alphas_))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10.0 0.75\n"
     ]
    }
   ],
   "source": [
    "print (clf_v2.alpha_, clf_v2.l1_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['34010.18575435   28358.83068991   10406.48101463    4579.38876158    237957.05647627\\n', '30921.21604569   27419.44834937    9601.86674996    8942.95226277    235101.15758987\\n', '30953.77101752   35297.70375882    9473.71739998    9787.78209156    227894.00071841\\n', '28185.07558869   33646.61283102    7590.73618035   14829.27974783    218610.94051325\\n', '26174.72335631   49034.82634789    7213.39643558   18641.01673012    191651.62338366\\n', '39714.34252438   43376.77380248   13421.39706272   38469.40537605    185517.9735573 \\n', '51067.86669252   40782.66137981   17916.44018973   53832.67461295    201642.60098661\\n', '34274.2352558    28522.76970872   10500.91887661    4429.00040861    237957.05647627\\n', '30939.22877798   27417.31794429    9604.19162707    8898.27953273    235659.5726195 \\n', '30927.97726823   34849.18465459    9490.71500407    9710.22516223    227944.22394889\\n', '28217.81436152   33476.27746559    7634.79261262   14790.59618771    218813.36464958\\n', '26255.04411672   50541.43921846    7213.11531469   18785.68601277    192063.06774424\\n', '40207.43568485   43275.79122468   14320.01292127   40301.91463741    186713.94026372\\n', '51632.00713531   42569.37376121   18823.89833834   54789.20615043    202985.06930614\\n', '34545.10067824   28700.54832661   10600.73870453    4299.31934419    237957.05647627\\n', '30957.30285433   27415.30928706    9606.56441054    8853.78470798    236222.43460828\\n', '30900.73703369   34401.68946104    9508.65852895    9628.60240844    227994.49051597\\n', '28218.64384864   33422.14468664    7670.41733472   15021.17895595    219026.30812486\\n', '26347.66570461   53322.95966236    7185.2896686    18927.97092969    192710.79398899\\n', '40532.75606831   44765.21129376   15419.41427701   42239.60965499    187508.55560982\\n', '51166.65420225   44949.60710384   20126.82390669   55051.24170714    204395.19665521\\n', '34822.86859978   28892.36723995   10706.01224105    4190.68288701    237957.05647627\\n', '30975.43835473   27413.422561      9608.98516637    8809.46809299    236789.75782159\\n', '30871.94986661   33955.67776316    9520.82336721    9577.99972849    228044.80043365\\n', '28186.34444983   33355.6949862     7723.23996114   15036.09096562    219347.88155343\\n', '26378.80107727   56614.50370558    7084.16468401   19181.04869844    193485.36768063\\n', '39797.78895546   46027.76362729   16880.45575642   44244.77560193    188145.45262939\\n', '50333.85888378   43182.22422338   22072.16969447   55247.30195664    207290.33137311\\n', '35107.62676985   29098.43012087   10816.81221382    4103.43379512    237957.05647627\\n', '30993.63535931   27411.65794968    9611.45396067    8765.32999278    237361.55657288\\n', '30863.85561064   33511.6718123     9533.81305842    9537.09824123    228095.15371592\\n', '28070.36474793   33242.99962548    7748.68934738   14935.2884887    219802.48810464\\n', '26285.95889404   57080.57084757    7048.00978768   19331.19356996    194264.53041036\\n', '37190.29607285   42244.76958858   19717.80664555   45319.94502048    190857.1860837 \\n', '47844.88240973   41982.98984095   26980.58675712   55569.06417506    210954.10614779\\n']\n"
     ]
    }
   ],
   "source": [
    "lines = []\n",
    "with open( \"regression/temp.txt\", 'r') as f:\n",
    "    lines = f.readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "alphas_ = [  1.00000000e+05 ,  1.00000000e+04 ,  1.00000000e+03   ,1.00000000e+02, 1.00000000e+01 ,  1.00000000e+00 ,  1.00000000e-01]\n",
    "mse_ =[]\n",
    "for line in lines:\n",
    "    temporino = [ float(item) for item in line.split()]\n",
    "    mse_.append(sum(temporino)/len(temporino))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63062.388539347994, 62397.328199532, 62681.394997257994, 60572.528972228, 58543.11725071199, 64099.978464586005, 73048.44877232399]\n",
      "[63136.796145201995, 62503.71810031399, 62584.46520760199, 60586.569055404005, 58971.670481376, 64963.818946386, 74159.910938286]\n"
     ]
    }
   ],
   "source": [
    "print(mse_[:7])\n",
    "print(mse_[7:14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 104893.56605105   48687.28049004   22598.63374615   10489.35660511\n",
      "    4868.728049      2259.86337462    1048.93566051     486.8728049\n",
      "     225.98633746     104.89356605]\n"
     ]
    }
   ],
   "source": [
    "with open (\"regression/elastic_net_cv10_l1r90_alpha.txt\", 'w') as f:\n",
    "    for alpha in alphas_:\n",
    "        f.write( \"\" + str(alpha) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mse_path_var = clf_v2.mse_path_\n",
    "mse1=[]\n",
    "mse2=[]\n",
    "mse3=[]\n",
    "mse_avg=[]\n",
    "#print (mse_path_)\n",
    "with open (\"regression/elastic_net_cv10_l1r90_var_mse.txt\", 'w') as f:\n",
    "    for c in mse_path_var:\n",
    "        f.write( str(c) + \"\\n\") \n",
    "        mse_avg.append( sum(c)/ len(c))\n",
    "#         f.write( str(cv1) + \",\" + str(cv2) + \",\"+str(cv3) + \"\\n\")\n",
    "#         mse1.append(cv1)\n",
    "#         mse2.append(cv2)\n",
    "#         mse3.append(cv3)\n",
    "#         mse_avg.append((cv1+cv2+cv3)/3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "coef_ = clf_v2.coef_\n",
    "len(coef_)\n",
    "with open (\"regression/elastic_net_cv10_l1r90_coef.txt\", 'w') as f:\n",
    "    for c in coef_:\n",
    "        f.write( \"\" + str(c) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#[0.75, 0.8, 0.85, 0.9, 0.95]\n",
    "plt.figure()\n",
    "\n",
    "plt.title('Elastic-Net MSE l1-ratios [0.75, 0.8, 0.85, 0.9, 0.95]')\n",
    "# plt.subplot(321)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[0:7]), ':' , label='0.75')\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# plt.subplot(322)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[7:14]), '--' , label='0.80')\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.80')\n",
    "# plt.subplot(323)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[14:21]), 'o-', label='0.85' )\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.85')\n",
    "# plt.subplot(324)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[21:28]), '.-', label='0.90' )\n",
    "\n",
    "# plt.xlabel('Log(alpha)')\n",
    "# plt.ylabel('Log(MSE)')\n",
    "# #plt.title('Elastic-Net MSE l1-ratio  0.90')\n",
    "# plt.subplot(325)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_[28:]), label='0.95' )\n",
    "\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Log(MSE)')\n",
    "#plt.title('Elastic-Net MSE l1-ratio  0.95')\n",
    "plt.legend()\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(2)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse2) )\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.figure(3)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse3) )\n",
    "\n",
    "plt.xlabel('-Log(alpha)')\n",
    "plt.ylabel('mse')\n",
    "plt.title('Lasso and Elastic-Net Paths')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mse_avg = [ sum(mse)/len(mse) for mse in clf_v2.mse_path_[1] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(4)\n",
    "plt.plot(np.log10(alphas_), np.log10(mse_avg) )\n",
    "plt.xlabel('Log(alpha)')\n",
    "plt.ylabel('Log(mse)')\n",
    "plt.title('Elastic-Net Parameter selection via log_10(MSE)')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(5)\n",
    "plt.plot(range(len(coef_)), coef_ )\n",
    "plt.xlabel('Feature Number')\n",
    "plt.ylabel('Weight')\n",
    "plt.title('Features')\n",
    "#plt.legend(l2, 'Elastic-Net', loc='lower left')\n",
    "#plt.axis('tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "c_list = []\n",
    "for i, c in enumerate(coef_):\n",
    "    c_list.append((i , c))\n",
    "\n",
    "sorted_c = sorted(c_list, key=lambda student: student[1]) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open (\"regression/elastic_net_cv3_l1r85_top100Words.txt\", 'w') as f:\n",
    "    for i in sorted_c[:50]:\n",
    "        f.write( \"{0:.5f}\".format( i[1]) + \"\\t\" +  Vinv [i[0]] + \"\\n\")    \n",
    "    for i in sorted_c[-50:]:\n",
    "        f.write ( \"{0:.5f}\".format( i[1])+ \"\\t\" +  Vinv [i[0]] + \"\\n\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(442, 24)\n",
      "Computing regularization path using the coordinate descent lasso...\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target\n",
    "\n",
    "rng = np.random.RandomState(42)\n",
    "X = np.c_[X, rng.randn(X.shape[0], 14)]  # add some bad features\n",
    "\n",
    "# normalize data as done by Lars to allow for comparison\n",
    "X /= np.sqrt(np.sum(X ** 2, axis=0))\n",
    "\n",
    "print (X.shape)\n",
    "\n",
    "# Compute paths\n",
    "print(\"Computing regularization path using the coordinate descent lasso...\")\n",
    "t1 = time.time()\n",
    "model = LassoCV(cv=20).fit(X, y)\n",
    "t_lasso_cv = time.time() - t1\n",
    "\n",
    "# Display results\n",
    "m_log_alphas = -np.log10(model.alphas_)\n",
    "\n",
    "plt.figure()\n",
    "ymin, ymax = 2300, 3800\n",
    "plt.plot(m_log_alphas, model.mse_path_, ':')\n",
    "plt.plot(m_log_alphas, model.mse_path_.mean(axis=-1), 'k',\n",
    "         label='Average across the folds', linewidth=2)\n",
    "plt.axvline(-np.log10(model.alpha_), linestyle='--', color='k',\n",
    "            label='alpha: CV estimate')\n",
    "\n",
    "plt.legend()\n",
    "\n",
    "plt.xlabel('-log(alpha)')\n",
    "plt.ylabel('Mean square error')\n",
    "plt.title('Mean square error on each fold: coordinate descent '\n",
    "          '(train time: %.2fs)' % t_lasso_cv)\n",
    "plt.axis('tight')\n",
    "plt.ylim(ymin, ymax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample size:  75\n",
      "top sample size:  75  top:  0.05\n",
      "(150, 184183)\n",
      "average acc: 0.51333, average precision: 0.50690, average recall: 1.00000\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  105\n",
      "top sample size:  105  top:  0.07\n",
      "(210, 184183)\n",
      "average acc: 0.57619, average precision: 0.54882, average recall: 0.88571\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  135\n",
      "top sample size:  135  top:  0.09\n",
      "(270, 184183)\n",
      "average acc: 0.58519, average precision: 0.56230, average recall: 0.78519\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  165\n",
      "top sample size:  165  top:  0.11\n",
      "(330, 184183)\n",
      "average acc: 0.58182, average precision: 0.57139, average recall: 0.67879\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  195\n",
      "top sample size:  195  top:  0.13\n",
      "(390, 184183)\n",
      "average acc: 0.58974, average precision: 0.58490, average recall: 0.61538\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  225\n",
      "top sample size:  225  top:  0.15\n",
      "(450, 184183)\n",
      "average acc: 0.56667, average precision: 0.56932, average recall: 0.54667\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  255\n",
      "top sample size:  255  top:  0.17\n",
      "(510, 184183)\n",
      "average acc: 0.55882, average precision: 0.57719, average recall: 0.48627\n",
      "average acc: 0.50000, average precision: 0.50000, average recall: 1.00000\n",
      "sample size:  285\n",
      "top sample size:  285  top:  0.19\n",
      "(570, 184183)\n",
      "average acc: 0.55789, average precision: 0.56292, average recall: 0.53333\n",
      "average acc: 0.49825, average precision: 0.49912, average recall: 0.99649\n",
      "sample size:  315\n",
      "top sample size:  315  top:  0.21\n",
      "(630, 184183)\n",
      "average acc: 0.55238, average precision: 0.56373, average recall: 0.47937\n",
      "average acc: 0.49841, average precision: 0.49920, average recall: 0.99683\n",
      "sample size:  345\n",
      "top sample size:  345  top:  0.23\n",
      "(690, 184183)\n",
      "average acc: 0.54058, average precision: 0.54868, average recall: 0.44928\n",
      "average acc: 0.49855, average precision: 0.49927, average recall: 0.99710\n",
      "sample size:  376\n",
      "top sample size:  376  top:  0.25\n",
      "(752, 184183)\n",
      "average acc: 0.56523, average precision: 0.59239, average recall: 0.42281\n",
      "average acc: 0.49600, average precision: 0.49796, average recall: 0.98933\n",
      "sample size:  406\n",
      "top sample size:  406  top:  0.27\n",
      "(812, 184183)\n",
      "average acc: 0.54315, average precision: 0.55711, average recall: 0.43098\n",
      "average acc: 0.50122, average precision: 0.50062, average recall: 0.99506\n",
      "sample size:  436\n",
      "top sample size:  436  top:  0.29\n",
      "(872, 184183)\n",
      "average acc: 0.54710, average precision: 0.56592, average recall: 0.40828\n",
      "average acc: 0.49885, average precision: 0.49940, average recall: 0.99080\n",
      "sample size:  466\n",
      "top sample size:  466  top:  0.31\n",
      "(932, 184183)\n",
      "average acc: 0.54182, average precision: 0.56246, average recall: 0.39039\n",
      "average acc: 0.50323, average precision: 0.50166, average recall: 0.99355\n",
      "sample size:  496\n",
      "top sample size:  496  top:  0.33\n",
      "(992, 184183)\n",
      "average acc: 0.54228, average precision: 0.56635, average recall: 0.36683\n",
      "average acc: 0.50404, average precision: 0.50207, average recall: 0.98590\n",
      "sample size:  526\n",
      "top sample size:  526  top:  0.35\n",
      "(1052, 184183)\n",
      "average acc: 0.54653, average precision: 0.57844, average recall: 0.33642\n",
      "average acc: 0.50288, average precision: 0.50149, average recall: 0.97721\n",
      "sample size:  556\n",
      "top sample size:  556  top:  0.37\n",
      "(1112, 184183)\n",
      "average acc: 0.55125, average precision: 0.59553, average recall: 0.32547\n",
      "average acc: 0.50539, average precision: 0.50282, average recall: 0.96942\n",
      "sample size:  586\n",
      "top sample size:  586  top:  0.39\n",
      "(1172, 184183)\n",
      "average acc: 0.56819, average precision: 0.55557, average recall: 0.74395\n",
      "average acc: 0.50512, average precision: 0.50262, average recall: 0.98803\n",
      "sample size:  616\n",
      "top sample size:  616  top:  0.41\n",
      "(1232, 184183)\n",
      "average acc: 0.55523, average precision: 0.54428, average recall: 0.69860\n",
      "average acc: 0.50407, average precision: 0.50212, average recall: 0.97402\n",
      "sample size:  646\n",
      "top sample size:  646  top:  0.43\n",
      "(1292, 184183)\n",
      "average acc: 0.56346, average precision: 0.55974, average recall: 0.70953\n",
      "average acc: 0.50852, average precision: 0.50443, average recall: 0.97835\n",
      "sample size:  676\n",
      "top sample size:  676  top:  0.45\n",
      "(1352, 184183)\n",
      "average acc: 0.55917, average precision: 0.54689, average recall: 0.68922\n",
      "average acc: 0.50814, average precision: 0.50421, average recall: 0.97633\n",
      "sample size:  706\n",
      "top sample size:  706  top:  0.47\n",
      "(1412, 184183)\n",
      "average acc: 0.57011, average precision: 0.55694, average recall: 0.68408\n",
      "average acc: 0.50428, average precision: 0.50221, average recall: 0.96323\n"
     ]
    }
   ],
   "source": [
    "#NB classification \n",
    "x_cord = []\n",
    "accuracies = []\n",
    "precision = []\n",
    "recall = []\n",
    "tf_accuracies = []\n",
    "tf_precision = []\n",
    "tf_recall = []\n",
    "#sample top/bottom rates\n",
    "for curr in np.arange(0.05, 0.48, 0.02):\n",
    "    topPercent = curr\n",
    "    (validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "    print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    data_tf = []\n",
    "    Yclass = []\n",
    "    Yreg = []\n",
    "\n",
    "    for i, docID in enumerate(validKeys):\n",
    "        for wordID in tf[docID]:\n",
    "            row.append(i)\n",
    "            col.append(wordID)\n",
    "            data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "            data_tf.append(tf[docID][wordID])\n",
    "    #     # bigram\n",
    "    #     for bigramID in bitf[docID]:\n",
    "    #         row.append(i)\n",
    "    #         col.append(VocabSize + bigramID)\n",
    "    #         data.append(bitf[docID][bigramID])\n",
    "\n",
    "    #     # uncomment to use regression\n",
    "        Yreg.append(locRates[docID]) \n",
    "        # used for classification\n",
    "        Yclass.append (labels[ locRates[docID] ])\n",
    "    X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    #bigram\n",
    "    #X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(trainIndices),VocabSize+len(VV.keys())), dtype=float)\n",
    "    #print (X.shape)\n",
    "\n",
    "    clf = MultinomialNB()\n",
    "    #clf = linear_model.SGDClassifier()\n",
    "    y = np.array(Yclass)\n",
    "    y_ridge = np.array(Yreg)\n",
    "    K = 5\n",
    "    acc = 0\n",
    "    prec = 0\n",
    "    recal = 0\n",
    "    acc_tf=0\n",
    "    prec_tf=0\n",
    "    recal_tf=0\n",
    "    k_fold = cross_validation.StratifiedKFold(Yclass, n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "    avg_ela = 0\n",
    "    terms = []\n",
    "    tf_terms = []\n",
    "    for k, (train, test) in enumerate(k_fold):\n",
    "        Y_hat = clf.fit(X[train], y[train]).predict(X[test])\n",
    "        Y_tf_hat = clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "#         ch2 = feature_selection.SelectKBest(feature_selection.chi2, k=150)\n",
    "#         X_new = ch2.fit_transform(X, y)\n",
    "#         #X_test = ch2.transform(X[test])\n",
    "#         Y_hat = clf.fit(X_new[train], y[train]).predict(X_new[test])\n",
    "#         if (k == 0 ):\n",
    "#             terms = [ Vinv[feature] for feature in ch2.get_support(indices=True) ] \n",
    "#             print (terms)\n",
    "            \n",
    "        #for feature in ch2.get_support(indices=True):\n",
    "        #\tprint ( Vinv[feature] )\n",
    "        \n",
    "#         X_tf_new = ch2.fit_transform(X_tf,y)\n",
    "#         Y_tf_hat = clf.fit(X_tf_new[train], y[train]).predict(X_tf_new[test])\n",
    "#         if (k == 0 ):\n",
    "#             terms_tf = [ Vinv[feature] for feature in ch2.get_support(indices=True) ] \n",
    "#             print (terms_tf)\n",
    "        \n",
    "        cm = conf_mat(Y_hat, y[test])\n",
    "        cm_tf = conf_mat(Y_tf_hat, y[test])\n",
    "        \n",
    "        acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "        prec = prec + cm[0]/max(cm[0]+cm[1],1)\n",
    "        recal = recal + cm[0]/max(cm[0]+cm[3],1)\n",
    "        \n",
    "        acc_tf = (cm_tf[0]+cm_tf[2])/(len(Y_tf_hat)) + acc_tf\n",
    "        prec_tf = prec_tf + cm_tf[0]/max(cm_tf[0]+cm_tf[1],1)\n",
    "        recal_tf = recal_tf + cm_tf[0]/max(cm_tf[0]+cm_tf[3],1)\n",
    "\n",
    "    print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc/K, prec/K, recal/K))\n",
    "    print (\"average acc: {0:.5f}, average precision: {1:.5f}, average recall: {2:.5f}\".format(acc_tf/K, prec_tf/K, recal_tf/K))\n",
    "    x_cord.append(topPercent)\n",
    "    accuracies.append(acc/K)\n",
    "    precision.append(prec/K)\n",
    "    recall.append(recal/K)\n",
    "    tf_accuracies.append(acc_tf/K)\n",
    "    tf_precision.append(prec_tf/K)\n",
    "    tf_recall.append(recal_tf/K)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "top sample size:  75  top:  0.05\n",
      "top sample size:  105  top:  0.07\n",
      "top sample size:  135  top:  0.09\n",
      "top sample size:  165  top:  0.11\n",
      "top sample size:  195  top:  0.13\n",
      "top sample size:  225  top:  0.15\n",
      "top sample size:  255  top:  0.17\n",
      "top sample size:  285  top:  0.19\n",
      "top sample size:  315  top:  0.21\n",
      "top sample size:  345  top:  0.23\n",
      "top sample size:  376  top:  0.25\n",
      "top sample size:  406  top:  0.27\n",
      "top sample size:  436  top:  0.29\n",
      "top sample size:  466  top:  0.31\n",
      "top sample size:  496  top:  0.33\n",
      "top sample size:  526  top:  0.35\n",
      "top sample size:  556  top:  0.37\n",
      "top sample size:  586  top:  0.39\n",
      "top sample size:  616  top:  0.41\n",
      "top sample size:  646  top:  0.43\n",
      "top sample size:  676  top:  0.45\n",
      "top sample size:  706  top:  0.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/squirtle/local/anaconda3/lib/python3.4/site-packages/sklearn/feature_selection/univariate_selection.py:150: RuntimeWarning: invalid value encountered in true_divide\n",
      "  chisq /= f_exp\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fe_accuracies = np.empty([22,40])\n",
    "fe_tf_accuracies = np.empty([22,40])\n",
    "#sample top/bottom rates\n",
    "for ia, curr in enumerate(np.arange(0.05, 0.48, 0.02)):\n",
    "    topPercent = curr\n",
    "    (validKeys, labels) = sampleItems(locRates, topPercent)\n",
    "    print (\"top sample size: \", (int)(topPercent*N), \" top: \", topPercent)\n",
    "    row = []\n",
    "    col = []\n",
    "    data = []\n",
    "    data_tf = []\n",
    "    Yclass = []\n",
    "    Yreg = []\n",
    "\n",
    "    for i, docID in enumerate(validKeys):\n",
    "        for wordID in tf[docID]:\n",
    "            row.append(i)\n",
    "            col.append(wordID)\n",
    "            data.append(tfidf(docID, wordID, tf,idf, N) ) \n",
    "            data_tf.append(tf[docID][wordID])\n",
    "\n",
    "        #     # uncomment to use regression\n",
    "        Yreg.append(locRates[docID]) \n",
    "        # used for classification\n",
    "        Yclass.append (labels[ locRates[docID] ])\n",
    "    X = csr_matrix ( (np.array(data),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    X_tf = csr_matrix ( (np.array(data_tf),(np.array(row),np.array(col))), shape=(len(validKeys),VocabSize), dtype=float)\n",
    "    \n",
    "    \n",
    "    clf = MultinomialNB()\n",
    "    #clf = linear_model.SGDClassifier()\n",
    "    y = np.array(Yclass)\n",
    "    #y_ridge = np.array(Yreg)\n",
    "    K = 5\n",
    "    k_fold = cross_validation.StratifiedKFold(Yclass, n_folds=K,shuffle=True, random_state=np.random.RandomState(seed))\n",
    "    #avg_ela = 0\n",
    "    #terms = []\n",
    "    #tf_terms = []\n",
    "    for ja, topFE in enumerate(range(100, 4100, 100)):\n",
    "        acc = 0\n",
    "        acc_tf=0\n",
    "        for k, (train, test) in enumerate(k_fold):\n",
    "            #Y_hat = clf.fit(X[train], y[train]).predict(X[test])\n",
    "            #Y_tf_hat = clf.fit(X_tf[train], y[train]).predict(X_tf[test])\n",
    "            ch2 = feature_selection.SelectKBest(feature_selection.chi2, k=topFE)\n",
    "            X_new = ch2.fit_transform(X, y)\n",
    "             #X_test = ch2.transform(X[test])\n",
    "            Y_hat = clf.fit(X_new[train], y[train]).predict(X_new[test])\n",
    "    \n",
    "            X_tf_new = ch2.fit_transform(X_tf,y)\n",
    "            Y_tf_hat = clf.fit(X_tf_new[train], y[train]).predict(X_tf_new[test])\n",
    "    \n",
    "            cm = conf_mat(Y_hat, y[test])\n",
    "            cm_tf = conf_mat(Y_tf_hat, y[test])\n",
    "\n",
    "            acc = (cm[0]+cm[2])/(len(Y_hat)) + acc\n",
    "            acc_tf = (cm_tf[0]+cm_tf[2])/(len(Y_tf_hat)) + acc_tf\n",
    "        \n",
    "        #x_cord.append(topPercent)\n",
    "        fe_accuracies[ia][ja] = acc/K\n",
    "#         precision.append(prec/K)\n",
    "#         recall.append(recal/K)\n",
    "        fe_tf_accuracies[ia][ja]=acc_tf/K\n",
    "#         tf_precision.append(prec_tf/K)\n",
    "#         tf_recall.append(recal_tf/K)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "temp = fe_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ttt = temp.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.59333333333333327"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttt[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xs = []\n",
    "ys = []\n",
    "for ia, curr in enumerate(np.arange(0.05, 0.48, 0.02)):\n",
    "    for ja, topFE in enumerate(range(100, 4100, 100)):\n",
    "        xs.append(((int)(curr*N))*2)\n",
    "        ys.append(topFE)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/squirtle/local/anaconda3/lib/python3.4/site-packages/matplotlib/axes/_axes.py:475: UserWarning: No labelled objects found. Use label='...' kwarg on individual plots.\n",
      "  warnings.warn(\"No labelled objects found. \"\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "#mpl.rcParams['title.fontsize'] = 10\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "ax.plot_trisurf(xs, ys, ttt, cmap=cm.jet, linewidth=0.2)\n",
    "ax.legend()\n",
    "\n",
    "ax.set_xlabel('Sample size')\n",
    "ax.set_ylabel('top K best features')\n",
    "ax.set_zlabel('Accuracy')\n",
    "ax.set_title('Naive Bayes Classification 5-fold Cross Validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open ('tables/acc_chi2_sampleSize.txt' , 'w') as f:\n",
    "    f.write(\"sampleSize, K, accuracy  \\n\")\n",
    "    for i in range(len(xs)):\n",
    "        #print (xs[i], ys[i], ttt[i])\n",
    "        f.write( \"{0:10d}, {1:10d}, {2:.3f}\\n\".format(xs[i], ys[i], ttt[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59333333,  0.63333333,  0.65333333,  0.64666667,  0.60666667,\n",
       "        0.65333333,  0.64666667,  0.64666667,  0.61333333,  0.58      ,\n",
       "        0.58666667,  0.58      ,  0.57333333,  0.59333333,  0.60666667,\n",
       "        0.61333333,  0.62      ,  0.62      ,  0.62      ,  0.62      ,\n",
       "        0.57142857,  0.5952381 ,  0.5952381 ,  0.6       ,  0.6047619 ,\n",
       "        0.5952381 ,  0.6047619 ,  0.60952381,  0.60952381,  0.6047619 ,\n",
       "        0.59047619,  0.59047619,  0.58571429,  0.5952381 ,  0.6047619 ,\n",
       "        0.58095238,  0.58571429,  0.5952381 ,  0.58571429,  0.5952381 ,\n",
       "        0.56666667,  0.59259259,  0.6037037 ,  0.59259259,  0.57037037,\n",
       "        0.57037037,  0.57407407,  0.57037037,  0.58148148,  0.58518519,\n",
       "        0.57407407,  0.57777778,  0.56666667,  0.56296296,  0.57037037,\n",
       "        0.55555556,  0.55185185,  0.57407407,  0.56666667,  0.57037037,\n",
       "        0.55454545,  0.55454545,  0.56969697,  0.56969697,  0.58484848,\n",
       "        0.57272727,  0.57575758,  0.58787879,  0.58484848,  0.57575758,\n",
       "        0.56969697,  0.57272727,  0.57575758,  0.57575758,  0.58484848,\n",
       "        0.59393939,  0.59393939,  0.60606061,  0.6030303 ,  0.61515152,\n",
       "        0.55641026,  0.56410256,  0.57692308,  0.55897436,  0.56666667,\n",
       "        0.56410256,  0.56666667,  0.55384615,  0.56666667,  0.56410256,\n",
       "        0.55641026,  0.55641026,  0.56666667,  0.56666667,  0.58717949,\n",
       "        0.59230769,  0.6       ,  0.6025641 ,  0.59487179,  0.60512821,\n",
       "        0.55111111,  0.56      ,  0.56888889,  0.56444444,  0.56888889,\n",
       "        0.56666667,  0.56888889,  0.56      ,  0.56222222,  0.56444444,\n",
       "        0.57333333,  0.56666667,  0.57333333,  0.57333333,  0.6       ,\n",
       "        0.59777778,  0.6       ,  0.61111111,  0.60444444,  0.60222222,\n",
       "        0.54509804,  0.54705882,  0.5627451 ,  0.55490196,  0.5627451 ,\n",
       "        0.56862745,  0.56666667,  0.55686275,  0.56078431,  0.56862745,\n",
       "        0.57254902,  0.5745098 ,  0.57843137,  0.57058824,  0.57058824,\n",
       "        0.59019608,  0.59607843,  0.59607843,  0.59019608,  0.59607843,\n",
       "        0.52280702,  0.52631579,  0.54210526,  0.55087719,  0.53684211,\n",
       "        0.54912281,  0.55614035,  0.55263158,  0.53859649,  0.56666667,\n",
       "        0.56491228,  0.53859649,  0.54035088,  0.56140351,  0.56491228,\n",
       "        0.55614035,  0.56666667,  0.5754386 ,  0.57368421,  0.57017544,\n",
       "        0.56190476,  0.55238095,  0.55714286,  0.55396825,  0.56984127,\n",
       "        0.55396825,  0.55396825,  0.55238095,  0.56349206,  0.56349206,\n",
       "        0.56349206,  0.55714286,  0.54920635,  0.54603175,  0.54920635,\n",
       "        0.56507937,  0.56984127,  0.57460317,  0.58888889,  0.58412698,\n",
       "        0.52463768,  0.53623188,  0.54492754,  0.54782609,  0.55652174,\n",
       "        0.55942029,  0.55797101,  0.56086957,  0.56086957,  0.56521739,\n",
       "        0.56231884,  0.57536232,  0.55362319,  0.55942029,  0.55652174,\n",
       "        0.56086957,  0.57971014,  0.58405797,  0.6057971 ,  0.6115942 ,\n",
       "        0.55589474,  0.57049123,  0.57180702,  0.56382456,  0.56115789,\n",
       "        0.56514035,  0.56115789,  0.56777193,  0.57578947,  0.57314035,\n",
       "        0.56512281,  0.55849123,  0.55580702,  0.58240351,  0.58373684,\n",
       "        0.58375439,  0.58505263,  0.58107018,  0.58112281,  0.58110526,\n",
       "        0.54196025,  0.54927733,  0.5504065 ,  0.56153267,  0.54918699,\n",
       "        0.55781391,  0.55534478,  0.5626769 ,  0.56273713,  0.56150256,\n",
       "        0.56150256,  0.5565944 ,  0.55779886,  0.57747666,  0.57380307,\n",
       "        0.56884974,  0.59598013,  0.58736826,  0.58610358,  0.59224631,\n",
       "        0.54474922,  0.55040491,  0.56645768,  0.55620428,  0.55967868,\n",
       "        0.55164577,  0.5550418 ,  0.54817137,  0.5596395 ,  0.56882184,\n",
       "        0.56765935,  0.5688349 ,  0.57341954,  0.58490073,  0.58948537,\n",
       "        0.58718652,  0.58939394,  0.58939394,  0.58369906,  0.57110763,\n",
       "        0.53002745,  0.53975063,  0.55575383,  0.54720888,  0.5568291 ,\n",
       "        0.5514642 ,  0.55788149,  0.55462137,  0.55141844,  0.55686342,\n",
       "        0.55363761,  0.55258522,  0.55472432,  0.56332647,  0.5782544 ,\n",
       "        0.58254404,  0.58790895,  0.5868108 ,  0.58358499,  0.58035919,\n",
       "        0.54737374,  0.54031313,  0.55043434,  0.54935354,  0.55240404,\n",
       "        0.55947475,  0.56349495,  0.55642424,  0.55141414,  0.56151515,\n",
       "        0.55546465,  0.56051515,  0.56856566,  0.56958586,  0.58070707,\n",
       "        0.57362626,  0.58367677,  0.58167677,  0.57863636,  0.58169697,\n",
       "        0.55886792,  0.54087152,  0.55039533,  0.54370171,  0.55036837,\n",
       "        0.55318958,  0.55131177,  0.55512129,  0.54654088,  0.54845463,\n",
       "        0.55508535,  0.5503504 ,  0.56746631,  0.56555256,  0.58459119,\n",
       "        0.57791554,  0.57884996,  0.57318958,  0.56842767,  0.57314465,\n",
       "        0.53420206,  0.53416184,  0.54228604,  0.54226995,  0.54860843,\n",
       "        0.55132722,  0.55041828,  0.5503861 ,  0.54766731,  0.55037001,\n",
       "        0.55217986,  0.55577542,  0.56747104,  0.5638594 ,  0.57199968,\n",
       "        0.56658623,  0.5638594 ,  0.56206564,  0.56565315,  0.56203346,\n",
       "        0.50511372,  0.50853977,  0.50596842,  0.50938722,  0.50767782,\n",
       "        0.50682312,  0.50938722,  0.50682312,  0.50767782,  0.50939447,\n",
       "        0.51622483,  0.51366073,  0.51366073,  0.51537013,  0.51280603,\n",
       "        0.51622483,  0.51707953,  0.51622483,  0.51707953,  0.51793423,\n",
       "        0.51292945,  0.5105101 ,  0.51616181,  0.51211644,  0.51049698,\n",
       "        0.51131655,  0.51293601,  0.51294256,  0.51538159,  0.51375557,\n",
       "        0.50892342,  0.50971676,  0.5097102 ,  0.50890375,  0.5064844 ,\n",
       "        0.50891031,  0.51212956,  0.51212956,  0.51616181,  0.51212956,\n",
       "        0.50772212,  0.51079308,  0.5092427 ,  0.50615981,  0.50923673,\n",
       "        0.51078116,  0.51308289,  0.51155635,  0.51462731,  0.51542636,\n",
       "        0.51542039,  0.52080501,  0.5177102 ,  0.5154025 ,  0.51539654,\n",
       "        0.51692904,  0.51847943,  0.52001193,  0.52077519,  0.51846154,\n",
       "        0.50666667,  0.51332244,  0.50888344,  0.51259259,  0.51037037,\n",
       "        0.51185185,  0.50962963,  0.51481481,  0.51703704,  0.51703159,\n",
       "        0.52221678,  0.51777233,  0.51703159,  0.51777233,  0.51925381,\n",
       "        0.51925381,  0.52147603,  0.52147603,  0.52221678,  0.52073529,\n",
       "        0.50070922,  0.50140845,  0.50423534,  0.50708221,  0.50779143,\n",
       "        0.50778643,  0.50778643,  0.50850065,  0.51205174,  0.50991909,\n",
       "        0.50850065,  0.50991909,  0.50991909,  0.51204675,  0.51205174,\n",
       "        0.5106333 ,  0.50992408,  0.50921486,  0.50991909,  0.51133753])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "mpl.rcParams['legend.fontsize'] = 10\n",
    "\n",
    "theta = np.linspace(-4 * np.pi, 4 * np.pi, 100)\n",
    "z = np.linspace(-2, 2, 100)\n",
    "r = z**2 + 1\n",
    "x = r * np.sin(theta)\n",
    "y = r * np.cos(theta)\n",
    "ax.plot(x, y, z, label='parametric curve')\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.2266666666666666"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fe_accuracies[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['your', 'no', 'today', ',', 'you', 'more', 'for', 'U', 'i', 'love', 'a', 'shit', \"don't\", 'know', 'why', '@', 'our', 'are', 'getting', '#', 'still', 'it', 'to', 'do', 'and', 'have', 'but', 'big', 'time', 'feel', 'all', 'the', 'of', 'is', 'right', 'in', 'real', 'back', 'on', 'at', '$', 'then', 'off', 'just', 'god', 'where', 'u', '&', 'made', 'by', 'that', 'always', 'was', \"i'm\", 'day', 'so', 'E', 'G', 'did', 'my', 'good', 'how', 'use', 'her', 'here', 'new', 'too', 'say', 'with', 'out', 'there', 'via', 'want', 'go', 'up', 'as', 'if', 'much', 'rt', 'well', 'not', \"it's\", 'this', 'great', 'has', 'or', 'check', 'who', 'any', 'will', 'never', 'get', 'been', 'night', 'think', 'she', 'stop', 'home', 'tonight', 'after', 'me', 'what', 'would', 'am', 'now', 'fuck', 'can', 'only', 'than', 'from', 'something', 'he', 'first', 'im', 'had', 'like', 'work', 'really', 'they', 'about', 'blue', 'black', 'come', 'free', \"that's\", 'us', 'be', 'make', 'lol', 'white', \"can't\", 'game', 'should', 'over', 'man', 'happy', 'year', 'him', 'world', 'everyone', 'watch', 'look', 'dress', 'some', 'because', 'twitter', 'an', 'got', 'week', 'when', 'we', 'these', 'them', 'better', 'way', 'video', 'snow', 'best', 'things', 'take', 'life', 'people', 'thanks', 'being', 'his', 'could', 'see', 'their', 'thank', \"i've\", 'ever', 'last', 'need', 'morning', 'down', \"you're\", 'someone', 'most', 'were', 'let', 'keep', 'every', 'looking', 'its', 'music', 'show', 'school', 'follow', 'win', 'even', 'going', 'may', 'live', 'photo', 'next', 'please', 're', 'into', 'tomorrow', 'help']\n"
     ]
    }
   ],
   "source": [
    "print (terms_tf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "accuracies_150 = accuracies\n",
    "accuracies_tf_150 = tf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idf_accuracies_no_filter = accuracies\n",
    "tf_accu_no_filter = tf_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, accuracies, label=\"tfidf+POS\")\n",
    "plt.plot(x_cord, tf_accuracies, label=\"tf+POS\")\n",
    "#plt.plot(x_cord, accuracies_150, label=\"tfidf + POS\")\n",
    "#plt.plot(x_cord, accuracies_tf_150, label=\"tf + POS\")\n",
    "plt.plot(x_cord, idf_accuracies_no_filter, label=\"tfidf\")\n",
    "plt.plot(x_cord, tf_accu_no_filter, label=\"tf\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classification')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, precision, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, tf_precision, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Precision')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, recall, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, tf_recall, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('Recall')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f1 = []\n",
    "f1_tf = []\n",
    "for p, r in zip(precision, recall):\n",
    "    f1.append( 2*(p*r)/(p+r))\n",
    "    \n",
    "for p, r in zip(tf_precision, tf_recall):\n",
    "    f1_tf.append( 2*(p*r)/(p+r))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open ('tables/multinomail_accuracy.txt', 'w') as f:\n",
    "    f.write(\"sample size, tfidf+POS, tf+POS, tfidf, tf  \\n\")\n",
    "    for i in range(len(x_cord)):\n",
    "        f.write( \"{0:10d}, {1:0.3f}, {2:.3f}, {3:.3f}, {4:.3f}\\n\".format((int)(x_cord[i]*N*2), accuracies[i], tf_accuracies[i], idf_accuracies_no_filter[i], tf_accu_no_filter[i]))\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in accuracies) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in tf_accuracies) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in idf_accuracies_no_filter) + '\\n')\n",
    "#         f.write( ','.join(format(x, \"2.3f\") for x in tf_accu_no_filter) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save for experimentation with the size of feature extraction\n",
    "accuracy_no_fe = accuracies\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[i for i in range(len(x_cord))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_cord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lines = []\n",
    "with open('tables/multinomail_accuracy.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "sampleSize = []\n",
    "tfidfPOS = []\n",
    "tfPOS = []\n",
    "tfidf = []\n",
    "tf = []\n",
    "for i, line in enumerate(lines):\n",
    "    if i > 0:\n",
    "        temp = line.strip().split(',')\n",
    "        sampleSize.append(int(temp[0]))\n",
    "        tfidfPOS.append(float(temp[1]))\n",
    "        tfPOS.append(float(temp[2]))\n",
    "        tfidf.append(float(temp[3]))\n",
    "        tf.append(float(temp[4]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(sampleSize, tfidfPOS, label=\"tfidf+POS\")\n",
    "plt.plot(sampleSize, tfPOS, label=\"tf+POS\")\n",
    "#plt.plot(x_cord, accuracies_150, label=\"tfidf + POS\")\n",
    "#plt.plot(x_cord, accuracies_tf_150, label=\"tf + POS\")\n",
    "plt.plot(sampleSize, tfidf, label=\"tfidf\")\n",
    "plt.plot(sampleSize, tf, label=\"tf\")\n",
    "plt.xlabel('total (training + testing) sample size ')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Naive Bayes Classifier 5-fold Cross Validation')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample size, tfidf+POS, tf+POS, tfidf, tf  \\n', '       150, 0.513, 0.500, 0.493, 0.500\\n', '       210, 0.576, 0.500, 0.476, 0.500\\n', '       270, 0.585, 0.500, 0.489, 0.500\\n', '       330, 0.582, 0.500, 0.500, 0.500\\n', '       391, 0.590, 0.500, 0.503, 0.500\\n', '       451, 0.567, 0.500, 0.513, 0.500\\n', '       511, 0.559, 0.500, 0.508, 0.500\\n', '       571, 0.558, 0.498, 0.514, 0.500\\n', '       631, 0.552, 0.498, 0.514, 0.500\\n', '       691, 0.541, 0.499, 0.514, 0.501\\n', '       752, 0.565, 0.496, 0.505, 0.499\\n', '       812, 0.543, 0.501, 0.512, 0.499\\n', '       872, 0.547, 0.499, 0.509, 0.499\\n', '       932, 0.542, 0.503, 0.506, 0.498\\n', '       992, 0.542, 0.504, 0.504, 0.498\\n', '      1052, 0.547, 0.503, 0.510, 0.502\\n', '      1112, 0.551, 0.505, 0.506, 0.502\\n', '      1173, 0.568, 0.505, 0.512, 0.502\\n', '      1233, 0.555, 0.504, 0.516, 0.498\\n', '      1293, 0.563, 0.509, 0.516, 0.498\\n', '      1353, 0.559, 0.508, 0.514, 0.501\\n', '      1413, 0.570, 0.504, 0.512, 0.500\\n']\n"
     ]
    }
   ],
   "source": [
    "print (lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(x_cord, f1, label=\"tfidf + POS filter\")\n",
    "plt.plot(x_cord, f1_tf, label=\"tf + POS filter\")\n",
    "plt.xlabel('top/botom n% HIV rates')\n",
    "plt.ylabel('F1')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import lasso_path, enet_path\n",
    "from sklearn import datasets\n",
    "\n",
    "diabetes = datasets.load_diabetes()\n",
    "X = diabetes.data\n",
    "y = diabetes.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(442, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
